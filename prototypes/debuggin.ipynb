{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import utils\n",
    "\n",
    "__author__ = \"Olivares Castillo José Luis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(file, threshold=0, vocabulary=None, dtype='float'):\n",
    "    # Copyright (C) 2016-2018  Mikel Artetxe <artetxem@gmail.com>\n",
    "    # https://github.com/artetxem/vecmap/blob/master/embeddings.py\n",
    "    \"\"\"Función para leer un archivo con los word embeddings.\n",
    "    Arguments:\n",
    "        file {str} -- archivo a leer.\n",
    "        threshold {int} -- Número a embeddings a leer\n",
    "                           (default:{El indicado en la cabecera del archivo})\n",
    "        vocabulary {str} -- Para solo acceder a vectores según el lexicon definido\n",
    "                            (default:{None})\n",
    "    \n",
    "    Return:\n",
    "        tuple -- lista de palabras y su correspondiente matriz de embeddings\n",
    "    \"\"\"\n",
    "    header = file.readline().split(' ')\n",
    "    count = int(header[0]) if threshold <= 0 else min(threshold, int(header[0]))\n",
    "    dim = int(header[1])\n",
    "    words = []\n",
    "    matrix = np.empty((count, dim), dtype=dtype) if vocabulary is None else []\n",
    "    for i in range(count):\n",
    "        word, vec = file.readline().split(' ', 1)\n",
    "        if vocabulary is None:\n",
    "            words.append(word)\n",
    "            matrix[i] = np.fromstring(vec, sep=' ', dtype=dtype)\n",
    "        elif word in vocabulary:\n",
    "            words.append(word)\n",
    "            matrix.append(np.fromstring(vec, sep=' ', dtype=dtype))\n",
    "    return (words, matrix) if vocabulary is None else (words, np.array(matrix, dtype=dtype))\n",
    "\n",
    "\n",
    "def closest_word_to(top_10, words):\n",
    "    \"\"\"Función para retornar las palabras de top_10 mediante su índice\n",
    "    Arguments:\n",
    "        top_10 {list} -- lista de tupla con índice una palabra y su distancia.\n",
    "        words {list} -- lista de palabras\n",
    "    Return:\n",
    "        list -- lista con palabras del top_10\n",
    "    \"\"\"\n",
    "    return [words[index] for index, _ in top_10]\n",
    "\n",
    "\n",
    "def get_lexicon(source):\n",
    "    \"\"\"Función para cargar lexicones\n",
    "    Arguments:\n",
    "        source {str} -- Nombre de archivo a cargar\n",
    "    Return:\n",
    "        list (2) -- lista con palabras del lexicon indicado.\n",
    "    \"\"\"\n",
    "    if source.__eq__(\"en-it.train\"):\n",
    "        src,trg = load_lexicon(\"../dataset/dictionaries/en-it.train.txt\")\n",
    "        return (src,trg)\n",
    "    elif source.__eq__(\"en-it.test\"):\n",
    "        src,trg = load_lexicon(\"../dataset/dictionaries/en-it.test.txt\")\n",
    "        return (src,trg)\n",
    "    elif source.__eq__(\"en-de.test\"):\n",
    "        src,trg = load_lexicon(\"../dataset/dictionaries/en-de.test.txt\")\n",
    "        return (src,trg)\n",
    "    elif source.__eq__(\"en-de.train\"):\n",
    "        src,trg = load_lexicon(\"../dataset/dictionaries/en-de.train.txt\")\n",
    "        return (src,trg)\n",
    "    elif source.__eq__(\"en-es.test\"):\n",
    "        src,trg = load_lexicon(\"../dataset/dictionaries/en-es.test.txt\")\n",
    "        return (src,trg)\n",
    "    elif source.__eq__(\"en-es.train\"):\n",
    "        src,trg = load_lexicon(\"../dataset/dictionaries/en-es.train.txt\")\n",
    "        return (src,trg)\n",
    "    elif source.__eq__(\"en-fi.test\"):\n",
    "        src,trg = load_lexicon(\"../dataset/dictionaries/en-fi.test.txt\")\n",
    "        return (src,trg)\n",
    "    elif source.__eq__(\"en-fi.train\"):\n",
    "        src,trg = load_lexicon(\"../dataset/dictionaries/en-fi.train.txt\")\n",
    "        return (src,trg)\n",
    "    else:\n",
    "        print(\"ERR: dataset inválido\")\n",
    "        return None,None\n",
    "    \n",
    "def load_lexicon(source):\n",
    "    \"\"\"Función auxiliar de `get_lexicon` para cargar lexicones\n",
    "    Arguments:\n",
    "        source {str} -- Nombre de archivo a cargar\n",
    "    Return:\n",
    "        list (2) -- lista con palabras del lexicon indicado.\n",
    "    \"\"\"\n",
    "    src, trg = list(), list()\n",
    "    with open(source, \"r\", encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            src.append(line.split()[0])\n",
    "            trg.append(line.split()[1])\n",
    "    return (src, trg)\n",
    "\n",
    "\n",
    "def get_vectors(lexicon, words, embeddings, dtype='float'):\n",
    "    \"\"\"Función para cargar vectores del lexicon indicado.\n",
    "    Arguments:\n",
    "        lexicon {list} -- lista de palabras del lexicon\n",
    "        words {list} -- lista con palabras de los vectores.\n",
    "        embeddings {numpy.ndarray} -- matriz con embeddings\n",
    "    Return:\n",
    "        numpy.ndarray -- Matriz con embeddings del lexicon\n",
    "    \"\"\"\n",
    "    matrix = np.empty((len(lexicon), embeddings.shape[1]), dtype=dtype)\n",
    "    for i in range(len(lexicon)):\n",
    "        if lexicon[i] in words:\n",
    "            matrix[i] = embeddings[words.index(lexicon[i])]\n",
    "    return np.asarray(matrix, dtype=dtype)\n",
    "\n",
    "\n",
    "def open_file(source):\n",
    "    \"\"\"Función para leer archivos\n",
    "    Arguments:\n",
    "        source {str} -- Archivo a leer\n",
    "    Return:\n",
    "        _io.TextIOWrapper -- Apuntador a fichero a leer\n",
    "    \"\"\"\n",
    "    if source.__eq__(\"en\"):\n",
    "        return open(\"../dataset/en.200k.300d.embeddings\",encoding=\"utf-8\", errors=\"surrogateescape\")\n",
    "    elif source.__eq__(\"it\"):\n",
    "        return open(\"../dataset/en-it/it.200k.300d.embeddings\",encoding=\"utf-8\", errors=\"surrogateescape\")\n",
    "    elif source.__eq__(\"es\"):\n",
    "        return open(\"../dataset/en-es/es.200k.300d.embeddings\",encoding=\"utf-8\", errors=\"surrogateescape\")\n",
    "    elif source.__eq__(\"de\"):\n",
    "        return open(\"../dataset/en-de/de.200k.300d.embeddings\",encoding=\"utf-8\", errors=\"surrogateescape\")\n",
    "    elif source.__eq__(\"fi\"):\n",
    "        return open(\"../dataset/en-fi/fi.200k.300d.embeddings\",encoding=\"utf-8\", errors=\"surr|ogateescape\")\n",
    "    else:\n",
    "        print(\"ERROR: dataset inválido\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revisar_palabras_duplicadas(source):\n",
    "    print(\"en-\"+source)\n",
    "    w1test, w2test = get_lexicon(\"en-\"+source+\".test\")\n",
    "    w1train, w2train = get_lexicon(\"en-\"+source+\".train\")\n",
    "    i=0\n",
    "    x=[]\n",
    "    print(\"w1...\")\n",
    "    for word in w1test:\n",
    "        if word in w1train:\n",
    "            i+=1\n",
    "            print(w2test.index(word),word,w2train[w2train.index(word)])\n",
    "    print(i,end=\"\\t\")\n",
    "    x = [w1test,w1train]\n",
    "    x = [set(a) for a in x]\n",
    "    f=set.intersection(*x)\n",
    "    print(f.__len__())\n",
    "    ##\n",
    "    i=0\n",
    "    x.clear()\n",
    "    print(\"w2...\")\n",
    "    for word in w2test:\n",
    "        if word in w2train:\n",
    "            i+=1\n",
    "            print(w2test.index(word),word,w2train[w2train.index(word)])\n",
    "    print(i,end=\"\\t\")\n",
    "    x = [w2test,w2train]\n",
    "    x = [set(a) for a in x]\n",
    "    f=set.intersection(*x)\n",
    "    print(f.__len__())\n",
    "    print(\"##################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en-it\n",
      "w1...\n",
      "0\t0\n",
      "w2...\n",
      "2 cucinare cucinare\n",
      "4 musicali musicali\n",
      "8 passeggeri passeggeri\n",
      "14 disposti disposti\n",
      "16 ultimi ultimi\n",
      "30 problema problema\n",
      "40 sono sono\n",
      "42 prodotto prodotto\n",
      "43 inchiesta inchiesta\n",
      "44 indagine indagine\n",
      "49 grande grande\n",
      "50 nominati nominati\n",
      "65 profitti profitti\n",
      "66 dichiarato dichiarato\n",
      "68 sufficiente sufficiente\n",
      "88 norme norme\n",
      "89 standard standard\n",
      "92 golfo golfo\n",
      "98 sole sole\n",
      "104 paura paura\n",
      "122 data data\n",
      "134 comunità comunità\n",
      "141 santa santa\n",
      "142 attori attori\n",
      "144 uccelli uccelli\n",
      "161 spazio spazio\n",
      "168 trasformazione trasformazione\n",
      "170 mantenere mantenere\n",
      "171 foto foto\n",
      "173 rubrica rubrica\n",
      "174 mai mai\n",
      "178 stessa stessa\n",
      "185 acquistato acquistato\n",
      "186 facoltà facoltà\n",
      "200 occidentale occidentale\n",
      "202 capo capo\n",
      "208 attività attività\n",
      "211 supplementari supplementari\n",
      "214 lord lord\n",
      "223 consentire consentire\n",
      "232 bancari bancari\n",
      "243 deve deve\n",
      "251 obiettivo obiettivo\n",
      "252 scopo scopo\n",
      "254 aiuto aiuto\n",
      "272 esterne esterne\n",
      "279 presidente presidente\n",
      "290 fondi fondi\n",
      "319 risposta risposta\n",
      "320 politica politica\n",
      "321 politiche politiche\n",
      "322 politici politici\n",
      "331 svolto svolto\n",
      "332 diritto diritto\n",
      "333 consigliere consigliere\n",
      "336 strade strade\n",
      "337 fine fine\n",
      "338 ordine ordine\n",
      "339 competenze competenze\n",
      "351 pensiero pensiero\n",
      "353 vantaggio vantaggio\n",
      "354 gatti gatti\n",
      "360 anche anche\n",
      "367 diverse diverse\n",
      "368 diversi diversi\n",
      "369 critica critica\n",
      "370 critiche critiche\n",
      "373 ulteriori ulteriori\n",
      "377 autobus autobus\n",
      "379 particolare particolare\n",
      "384 dichiarazione dichiarazione\n",
      "386 risposte risposte\n",
      "387 segue segue\n",
      "390 responsabilità responsabilità\n",
      "398 più più\n",
      "423 sostenere sostenere\n",
      "424 ampiamente ampiamente\n",
      "429 prova prova\n",
      "452 gas gas\n",
      "465 bestiame bestiame\n",
      "473 costosa costosa\n",
      "475 costosi costosi\n",
      "476 costoso costoso\n",
      "540 riscaldamento riscaldamento\n",
      "549 firma firma\n",
      "550 immigrazione immigrazione\n",
      "564 ferite ferite\n",
      "565 mentali mentali\n",
      "632 pressioni pressioni\n",
      "653 veterinari veterinari\n",
      "660 affidabilità affidabilità\n",
      "661 fiscale fiscale\n",
      "662 fiscali fiscali\n",
      "686 consultivo consultivo\n",
      "690 aule aule\n",
      "710 affitto affitto\n",
      "723 caratteristica caratteristica\n",
      "738 luna luna\n",
      "757 vicino vicino\n",
      "771 convinzioni convinzioni\n",
      "823 imprese imprese\n",
      "854 ideatori ideatori\n",
      "993 piana piana\n",
      "1042 sia sia\n",
      "1127 incatenato incatenato\n",
      "1142 marino marino\n",
      "1283 olandesi olandesi\n",
      "1429 accuratezza accuratezza\n",
      "1495 fascicoli fascicoli\n",
      "1545 vecchi vecchi\n",
      "1580 segni segni\n",
      "1742 russa russa\n",
      "1757 marco marco\n",
      "113\t113\n",
      "##################\n",
      "en-de\n",
      "w1...\n",
      "0\t0\n",
      "w2...\n",
      "1 unterschied unterschied\n",
      "11 allgemeine allgemeine\n",
      "23 zimmern zimmern\n",
      "33 sitz sitz\n",
      "55 ergebnis ergebnis\n",
      "58 kaum kaum\n",
      "59 schiff schiff\n",
      "60 gemischten gemischten\n",
      "82 arbeitnehmer arbeitnehmer\n",
      "83 sachwalter sachwalter\n",
      "103 verweigert verweigert\n",
      "105 stimmen stimmen\n",
      "109 große große\n",
      "114 letzte letzte\n",
      "119 lehrer lehrer\n",
      "131 personal personal\n",
      "134 verleger verleger\n",
      "141 konkrete konkrete\n",
      "155 begriff begriff\n",
      "169 verbrechen verbrechen\n",
      "172 tests tests\n",
      "179 schreiben schreiben\n",
      "182 gewinnen gewinnen\n",
      "187 rundfunk rundfunk\n",
      "190 kluft kluft\n",
      "196 hügel hügel\n",
      "204 laufbahn laufbahn\n",
      "205 karriere karriere\n",
      "207 betroffenen betroffenen\n",
      "216 anschließend anschließend\n",
      "222 fliegen fliegen\n",
      "227 kapazitäten kapazitäten\n",
      "228 fähigkeiten fähigkeiten\n",
      "234 fragen fragen\n",
      "237 computer computer\n",
      "241 ständigen ständigen\n",
      "247 redner redner\n",
      "248 größeren größeren\n",
      "252 sie sie\n",
      "262 wort wort\n",
      "276 innere innere\n",
      "278 versorgung versorgung\n",
      "282 beteiligung beteiligung\n",
      "290 grundlegende grundlegende\n",
      "291 grundlegenden grundlegenden\n",
      "296 zeitschrift zeitschrift\n",
      "330 jeder jeder\n",
      "332 jedes jedes\n",
      "334 wissenschaftler wissenschaftler\n",
      "338 anhörung anhörung\n",
      "353 bilder bilder\n",
      "357 krank krank\n",
      "370 anstrengungen anstrengungen\n",
      "377 erfahrungen erfahrungen\n",
      "407 fast fast\n",
      "408 nahezu nahezu\n",
      "423 kredite kredite\n",
      "429 kapitel kapitel\n",
      "432 gut gut\n",
      "441 einklang einklang\n",
      "451 gastgeber gastgeber\n",
      "458 pflichten pflichten\n",
      "492 architekten architekten\n",
      "507 pistole pistole\n",
      "555 nassen nassen\n",
      "590 effizient effizient\n",
      "638 bewertet bewertet\n",
      "654 lehrpläne lehrpläne\n",
      "695 reden reden\n",
      "710 kopf kopf\n",
      "714 moore moore\n",
      "755 blogs blogs\n",
      "761 kleinsten kleinsten\n",
      "766 würde würde\n",
      "828 trost trost\n",
      "863 umstrittenen umstrittenen\n",
      "1054 homosexuelle homosexuelle\n",
      "1175 camino camino\n",
      "1344 ruf ruf\n",
      "1676 knochenmehl knochenmehl\n",
      "1828 sauren sauren\n",
      "1833 verfassern verfassern\n",
      "82\t82\n",
      "##################\n",
      "en-fi\n",
      "w1...\n",
      "0\t0\n",
      "w2...\n",
      "7 ryhmän ryhmän\n",
      "17 oppineet oppineet\n",
      "42 autojen autojen\n",
      "59 ruotsi ruotsi\n",
      "60 ruotsin ruotsin\n",
      "74 kanadan kanadan\n",
      "82 poika poika\n",
      "88 puun puun\n",
      "89 puuta puuta\n",
      "97 sopimus sopimus\n",
      "110 suoraan suoraan\n",
      "129 monimutkaisia monimutkaisia\n",
      "130 monimutkainen monimutkainen\n",
      "144 aasian aasian\n",
      "145 toimija toimija\n",
      "146 toimijana toimijana\n",
      "151 sairaaloiden sairaaloiden\n",
      "154 vaatimuksia vaatimuksia\n",
      "155 vaatimukset vaatimukset\n",
      "159 puhdasta puhdasta\n",
      "164 juutalaisten juutalaisten\n",
      "165 juutalaisia juutalaisia\n",
      "189 tarkkoja tarkkoja\n",
      "190 tuloja tuloja\n",
      "237 elokuva elokuva\n",
      "238 elokuvan elokuvan\n",
      "257 muut muut\n",
      "260 toinen toinen\n",
      "261 iloinen iloinen\n",
      "262 sisältää sisältää\n",
      "263 työpaikkojen työpaikkojen\n",
      "270 yritys yritys\n",
      "292 australian australian\n",
      "304 ovat ovat\n",
      "307 hyvin hyvin\n",
      "315 vain vain\n",
      "319 käsikirjaa käsikirjaa\n",
      "322 arviointia arviointia\n",
      "323 arvioinnin arvioinnin\n",
      "344 osoittaneet osoittaneet\n",
      "345 osoittanut osoittanut\n",
      "353 paljon paljon\n",
      "357 espanja espanja\n",
      "358 espanjan espanjan\n",
      "365 koska koska\n",
      "423 venäjän venäjän\n",
      "433 heti heti\n",
      "454 tyytyväinen tyytyväinen\n",
      "460 säännöllisesti säännöllisesti\n",
      "496 tehokkuuden tehokkuuden\n",
      "497 tehokkuutta tehokkuutta\n",
      "506 tutkijoiden tutkijoiden\n",
      "507 tutkijat tutkijat\n",
      "544 joka joka\n",
      "567 lawrence lawrence\n",
      "691 matematiikkaa matematiikkaa\n",
      "692 matematiikassa matematiikassa\n",
      "699 järjestelmällisesti järjestelmällisesti\n",
      "734 sveitsin sveitsin\n",
      "764 pakattu pakattu\n",
      "802 hoitajan hoitajan\n",
      "814 väliaikaisen väliaikaisen\n",
      "918 lämpötila lämpötila\n",
      "923 avoin avoin\n",
      "996 investointien investointien\n",
      "997 investointeja investointeja\n",
      "999 johannes johannes\n",
      "1003 rahoitus rahoitus\n",
      "1044 ilmatilan ilmatilan\n",
      "1045 ilmatilaa ilmatilaa\n",
      "1345 foorumeilla foorumeilla\n",
      "1435 lahteen lahteen\n",
      "1573 suolan suolan\n",
      "1596 koomista koomista\n",
      "1645 san san\n",
      "1833 alin alin\n",
      "1982 kruunu kruunu\n",
      "77\t77\n",
      "##################\n",
      "en-es\n",
      "w1...\n",
      "0\t0\n",
      "w2...\n",
      "14 especialmente especialmente\n",
      "15 garantizar garantizar\n",
      "20 archivos archivos\n",
      "21 función función\n",
      "34 lucha lucha\n",
      "38 invitado invitado\n",
      "39 escrito escrito\n",
      "42 dama dama\n",
      "46 básicamente básicamente\n",
      "50 agente agente\n",
      "51 actor actor\n",
      "62 pedido pedido\n",
      "65 modo modo\n",
      "66 forma forma\n",
      "67 asociación asociación\n",
      "80 comienza comienza\n",
      "81 empieza empieza\n",
      "96 animales animales\n",
      "99 canal canal\n",
      "101 mantener mantener\n",
      "109 hogares hogares\n",
      "110 casas casas\n",
      "113 medida medida\n",
      "116 clásica clásica\n",
      "117 clásico clásico\n",
      "118 prueba prueba\n",
      "133 climático climático\n",
      "146 carrera carrera\n",
      "151 proveedor proveedor\n",
      "152 comenzar comenzar\n",
      "153 empezar empezar\n",
      "154 imagen imagen\n",
      "155 trenes trenes\n",
      "158 responsabilidad responsabilidad\n",
      "165 papeles papeles\n",
      "166 libros libros\n",
      "171 pacientes pacientes\n",
      "178 útil útil\n",
      "183 adicional adicional\n",
      "184 barcos barcos\n",
      "185 buques buques\n",
      "118 prueba prueba\n",
      "200 ganar ganar\n",
      "202 temo temo\n",
      "203 miedo miedo\n",
      "204 musulmanes musulmanes\n",
      "205 señales señales\n",
      "208 carácter carácter\n",
      "212 conducta conducta\n",
      "214 profesor profesor\n",
      "216 empresas empresas\n",
      "218 este este\n",
      "228 exterior exterior\n",
      "229 pruebas pruebas\n",
      "234 centro centro\n",
      "236 cinematográfica cinematográfica\n",
      "237 cine cine\n",
      "240 norma norma\n",
      "245 escuela escuela\n",
      "257 billetes billetes\n",
      "263 adecuados adecuados\n",
      "272 significa significa\n",
      "257 billetes billetes\n",
      "290 esperar esperar\n",
      "292 mayor mayor\n",
      "298 aplicación aplicación\n",
      "307 hecho hecho\n",
      "309 asociaciones asociaciones\n",
      "310 hermano hermano\n",
      "312 teléfonos teléfonos\n",
      "317 actual actual\n",
      "319 cristianos cristianos\n",
      "321 comercial comercial\n",
      "322 comerciales comerciales\n",
      "325 más más\n",
      "334 normalmente normalmente\n",
      "335 combustible combustible\n",
      "342 independientemente independientemente\n",
      "353 últimos últimos\n",
      "361 armas armas\n",
      "363 madera madera\n",
      "375 votar votar\n",
      "376 voto voto\n",
      "393 estadística estadística\n",
      "395 relaciones relaciones\n",
      "405 vehículos vehículos\n",
      "419 química química\n",
      "440 matriz matriz\n",
      "445 cuestión cuestión\n",
      "448 pintura pintura\n",
      "451 impresos impresos\n",
      "452 observaciones observaciones\n",
      "455 relación relación\n",
      "456 todo todo\n",
      "462 expertos expertos\n",
      "465 disposiciones disposiciones\n",
      "470 aumentar aumentar\n",
      "472 intervención intervención\n",
      "489 detenido detenido\n",
      "490 detenidos detenidos\n",
      "536 contractual contractual\n",
      "537 iraquíes iraquíes\n",
      "557 lógica lógica\n",
      "582 observación observación\n",
      "363 madera madera\n",
      "614 plaza plaza\n",
      "657 entornos entornos\n",
      "670 delicado delicado\n",
      "714 operaciones operaciones\n",
      "726 eliminación eliminación\n",
      "782 catástrofe catástrofe\n",
      "785 lobos lobos\n",
      "786 grises grises\n",
      "787 escoceses escoceses\n",
      "789 cualificados cualificados\n",
      "794 registro registro\n",
      "830 apenas apenas\n",
      "848 importes importes\n",
      "850 cantidades cantidades\n",
      "852 canadienses canadienses\n",
      "855 reina reina\n",
      "1046 dar dar\n",
      "1084 excelencia excelencia\n",
      "1148 moreno moreno\n",
      "1158 republicano republicano\n",
      "1632 corona corona\n",
      "1710 apartado apartado\n",
      "127\t124\n",
      "##################\n"
     ]
    }
   ],
   "source": [
    "revisar_palabras_duplicadas(\"it\")\n",
    "revisar_palabras_duplicadas(\"de\")\n",
    "revisar_palabras_duplicadas(\"fi\")\n",
    "revisar_palabras_duplicadas(\"es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def get_vectors(lexicon, words, embeddings, dtype='float'):\n",
    "    matrix = np.empty((len(lexicon), embeddings.shape[1]), dtype=dtype)\n",
    "    tmp=[]\n",
    "    for i in range(len(lexicon)):\n",
    "        if lexicon[i] in words:\n",
    "            matrix[i] = embeddings[words.index(lexicon[i])]\n",
    "            tmp.append(words.index(lexicon[i]))\n",
    "    return np.asarray(matrix, dtype=dtype),tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3442, 300)\n"
     ]
    }
   ],
   "source": [
    "words_scr_lexicon, words_trg_lexicon = get_lexicon(\"en-it.train\")\n",
    "source_vec = open_file('en')\n",
    "words_src, source_vec = read(source_vec)\n",
    "eval_src = list(set(words_scr_lexicon))\n",
    "src_vec = get_vectors(eval_src, words_src, source_vec)\n",
    "print(src_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 300)\n"
     ]
    }
   ],
   "source": [
    "words_scr_lexicont, words_trg_lexicont = get_lexicon(\"en-it.test\")\n",
    "source_vect = open_file('en')\n",
    "words_srct, source_vect = read(source_vect)\n",
    "eval_srct = list(set(words_scr_lexicont))\n",
    "src_vect = get_vectors(eval_srct, words_srct, source_vect)\n",
    "print(src_vect.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(src_vect[0],src_vect[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "CPU times: user 43.1 s, sys: 4.04 ms, total: 43.1 s\n",
      "Wall time: 43.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "p=0\n",
    "ai,aj=[],[]\n",
    "for i in range(len(src_vect)):\n",
    "    for j in range(len(src_vec)):\n",
    "        if np.array_equal(src_vect[i],src_vec[j]):\n",
    "            p+=1\n",
    "            ai.append(i)\n",
    "            aj.append(j)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trg_vec = open_file('en')\n",
    "words_trg,src_vec = read(trg_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, index,counts = np.unique(src_vec, return_index=True,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words_src),len(set(words_src))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique.shape,index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_src = list(set(w2test))\n",
    "src_vec,tmp = get_vectors(eval_src, words_src, source_vec)\n",
    "print(src_vec.shape)\n",
    "\n",
    "eval_src = list(set(w2train))\n",
    "src_vec1,tmp1 = get_vectors(eval_src, words_src, source_vec)\n",
    "\n",
    "print(src_vec1.shape)\n",
    "x = [tmp,tmp1]\n",
    "x = [set(a) for a in x]\n",
    "f=set.intersection(*x)\n",
    "print(\"intersection\")\n",
    "print(f.__len__())\n",
    "if f:\n",
    "    print('lista')\n",
    "    f=list(f)\n",
    "    print(words_src[f[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.__len__(),tmp1.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sys import argv\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "__author__ = \"Olivares Castillo José Luis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es, na = utils.load_embeddings(\"n2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_dummy = na.drop(na.columns[0], axis=1)\n",
    "type(na_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_dummy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_vectores1 = na_dummy.values.astype(np.float64)\n",
    "na_vectores1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eval_set = utils.get_lexicon(\"eval\")\n",
    "eval_es = list(set(eval_set[\"esp\"]))\n",
    "eval_es_index = [int(es[es[0] == palabra].index[0])\n",
    "                 for palabra in eval_es]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_es_vectores = utils.get_vectors(es, eval_es_index)\n",
    "test_vectors = np.array([np.array(es.iloc[indice][1::]).astype(np.float64) for indice in eval_es_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_es_vectores = utils.get_vectors(es, eval_es_index)\n",
    "test_vectors = np.array([np.array(es.iloc[indice][1::]).astype(np.float64) for indice in eval_es_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "#saver = tf.train.import_meta_graph('./models/model1111_gpu/model2250.ckpt.meta')\n",
    "#saver.restore(sess, tf.train.latest_checkpoint('./models/model1111_gpu/'))\n",
    "saver = tf.train.import_meta_graph('./models/model_joyce/modeljoyce.ckpt.meta')\n",
    "saver.restore(sess, tf.train.latest_checkpoint('./models/model_joyce/'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.get_default_graph()\n",
    "X = graph.get_tensor_by_name(\"input/input_es:0\")\n",
    "#y = graph.get_tensor_by_name(\"input/target_na:0\")\n",
    "kprob = graph.get_tensor_by_name(\"dropout_prob:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_NN = graph.get_tensor_by_name(\"xw_plus_b_1:0\")\n",
    "#output_NN = graph.get_tensor_by_name(\"dense_2/BiasAdd:0\")\n",
    "#output_NN = graph.get_tensor_by_name(\"output_1:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict = {X: test_vectors, kprob: 1}\n",
    "pred = sess.run(output_NN, feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10 = [utils.get_top10_closest(pred[_], na_vectores1)\n",
    "          for _ in range(pred.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest = [utils.get_closest_words_to(top_10[_], na)\n",
    "           for _ in range(pred.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados = {palabra_es: top_10_nah for (palabra_es, top_10_nah) in zip(eval_es, closest)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esp = list(eval_set[\"esp\"].values)\n",
    "nah = list(eval_set[\"nah\"].values)\n",
    "pares_eval = list(zip(esp, nah))\n",
    "gold = defaultdict(list)\n",
    "for palabra_es, palabra_na in pares_eval:\n",
    "    gold[palabra_es].append(palabra_na)\n",
    "gold = dict(gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = 0\n",
    "p5 = 0\n",
    "p10 = 0\n",
    "list_esp_eval = (list(resultados.keys()))\n",
    "hits = list()\n",
    "not_found = list()\n",
    "# Se buscan las traducciones gold standard dentro de las predicciones y se obtiene \n",
    "# P@K, sino se encuentran, se añade a una lista de no encontrados.\n",
    "for palabra_gold in list_esp_eval:\n",
    "    for i in gold[palabra_gold]:\n",
    "        if i in resultados[palabra_gold]:\n",
    "            hits.append(resultados[palabra_gold].index(i))\n",
    "    if hits.__len__() > 0:\n",
    "        if min(hits) == 0:\n",
    "            p1 += 1\n",
    "            p5 += 1\n",
    "            p10 += 1\n",
    "        if min(hits) >= 1 and min(hits) <= 5:\n",
    "            p5 += 1\n",
    "            p10 += 1\n",
    "        if min(hits) > 5 and min(hits) <= 10:\n",
    "            p10 += 1\n",
    "    else:\n",
    "        not_found.append(palabra_gold)\n",
    "    hits.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = list_esp_eval.__len__()\n",
    "print(\"not found:\", not_found.__len__(), \"-\", not_found.__len__() / length, \"%\")\n",
    "print(\"P@1:\", p1,\"\\tP@5:\", p5 , \"\\tP@10:\", p10)\n",
    "print(\"P@1:\", p1 / length,\"\\tP@5:\", p5 / length, \"\\tP@10:\", p10 / length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
