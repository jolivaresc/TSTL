{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import utils\n",
    "__author__ = \"Olivares Castillo José Luis\"\n",
    "\n",
    "# reset everything to rerun in jupyter\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow v1.5.0-rc0\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow v{}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semilla para reproducibilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargar vectores desde archivos.\n",
    "Leer archivos node2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "es: (4433, 129) \tna: (1904, 129)\n"
     ]
    }
   ],
   "source": [
    "es, na = utils.load_node2vec()\n",
    "print(\"es:\",es.shape,\"\\tna:\",na.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se buscan los índices de los lexicones semilla dentro de los dataframes para poder acceder a sus representaciones vectoriales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index_es: 540 index_na: 540\n"
     ]
    }
   ],
   "source": [
    "index_es, index_na = utils.get_seed_index(es,na)\n",
    "print(\"index_es:\",index_es.__len__(),\"index_na:\",index_na.__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se obtienen los vectores de los lexicones semilla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "es_vectores = utils.get_vectors(es,index_es)\n",
    "na_vectores = utils.get_vectors(na,index_na)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.7\n",
    "\n",
    "# Dimensión de vectores de entrada (número de neuronas en capa de entrada).\n",
    "NODES_INPUT = es_vectores[0].size   \n",
    "\n",
    "# Número de neuronas en capas ocultas.\n",
    "NODES_H1 = 70                       \n",
    "NODES_H2 = 42\n",
    "NODES_H3 = 70\n",
    "\n",
    "# (número de neuronas en capa de entrada).\n",
    "NODES_OUPUT = na_vectores[0].size\n",
    "\n",
    "\n",
    "EPOCHS = 100000\n",
    "\n",
    "# Ruta donde se guarda el grafo para visualizar en TensorBoard.\n",
    "LOGPATH = utils.make_hparam_string(\"MSE\",\"RELU\",\"Adagrad\",\"H\",NODES_H1,\n",
    "                                   NODES_H2,NODES_H3,\"LR\",LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Placeholders\n",
    "\n",
    "Tensores donde estarán los vectores de entrada y salida.\n",
    "\n",
    "* X: Vectores de español.\n",
    "* y: Vectores de náhuatl.\n",
    "\n",
    "`tf.name_scope` se utiliza para mostrar las entradas del grafo computacional en `TensorBoard`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('input'):\n",
    "    # El valor None indica que se puede modificar la dimensión de los tensores\n",
    "    # por si se usan todos los vectores o batches.\n",
    "    X = tf.placeholder(shape=[None, NODES_INPUT],dtype=tf.float64, name='input_es')\n",
    "    y = tf.placeholder(shape=[None, NODES_OUPUT],dtype=tf.float64, name='target_na')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Función para crear las capas de la red.\n",
    "\n",
    "\n",
    "Función para crear capas.\n",
    "\n",
    "Args:\n",
    "* input (Tensor): Tensor de entrada a la capa.\n",
    "* size_in, size_out (int): Dimensiones de entrada y salida de la capa.\n",
    "* name (str): Nombre de la capa. Default: fc.\n",
    "* stddev (float): Desviación estándar con la que se inicializan los pesos de la capa.\n",
    "* dtype: Floating-point representation.\n",
    "\n",
    "Returns:\n",
    "* act (Tensor): $(input * weights) + bias $\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fully_connected_layer(input, size_in, size_out, name = \"fc\", stddev=0.1,\n",
    "                          dtype = tf.float64):\n",
    "    with tf.name_scope(name):\n",
    "        # Tensor de pesos.\n",
    "        W = tf.Variable(tf.truncated_normal([size_in, size_out], stddev=stddev,\n",
    "                                            dtype=dtype), name=\"W\")\n",
    "        # Bias.\n",
    "        b = tf.Variable(tf.constant(0.1, shape=[size_out], dtype = dtype), name=\"b\")\n",
    "        \n",
    "        # Realiza la operación input * + b (tf.nn.xw_plus_b)\n",
    "        act = tf.add(tf.matmul(input,W), b)\n",
    "        \n",
    "        # Se generan histogramas de los pesos y la salida de la capa para poder\n",
    "        # visualizarlos en TensorBoard.\n",
    "        tf.summary.histogram(\"weights\", W)\n",
    "        #tf.summary.histogram(\"biases\", b)\n",
    "        tf.summary.histogram(\"activations\", act)\n",
    "        \n",
    "        return act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activación de capas.\n",
    "Función para activar la salida de las capas.\n",
    "\n",
    "Args:\n",
    "* layer (Tensor): Capa que será activada.\n",
    "* name (string): Nombre de la capa para mostrar en `TensorBoard`.\n",
    "* act (string): Función de activación. Default: [ReLU](https://www.tensorflow.org/api_docs/python/tf/nn/relu). También se pueden utilizar [Leaky ReLU](https://www.tensorflow.org/api_docs/python/tf/nn/leaky_relu) con un parámetro `alpha = 0.2` por defecto y [Softmax](https://www.tensorflow.org/api_docs/python/tf/nn/softmax) para la capa de salida.\n",
    "\n",
    "Returns:\n",
    "    Capa con función de activación aplicada.\n",
    "    \n",
    "**NOTA:**\n",
    ">3.4 Why do we use a leaky ReLU and not a ReLU as an activation function?\n",
    "We want gradients to flow while we backpropagate through the network. \n",
    "We stack many layers in a system in which there are some neurons \n",
    "whose value drop to zero or become negative. Using a ReLU as an activation \n",
    "function clips the negative values to zero and in the backward pass, \n",
    "the gradients do not flow through those neurons where the values become zero. \n",
    "Because of this the weights do not get updated, and the network stops learning \n",
    "for those values. So using ReLU is not always a good idea. However, we encourage \n",
    "you to change the activation function to ReLU and see the difference.\n",
    "[See link](https://www.learnopencv.com/understanding-autoencoders-using-tensorflow-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def activation_function(layer, act, name, alpha = tf.constant(0.2, dtype=tf.float64)):\n",
    "    if act == \"leaky_relu\":\n",
    "        return tf.nn.leaky_relu(layer, alpha, name = name)\n",
    "    elif act == \"softmax\":\n",
    "        return tf.nn.softmax(layer, name = name)\n",
    "    return tf.nn.relu(layer, name = name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se definen las capas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'fc1/relu:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se calcula la salida de la capa.\n",
    "fc1 = fully_connected_layer(X,NODES_INPUT,NODES_H1)\n",
    "\n",
    "# Activación de la capa.\n",
    "fc1 = activation_function(fc1, \"relu\", \"fc1\")\n",
    "\n",
    "# Se añade histograma de activación de la capa para visualizar en\n",
    "# TensorBoard.\n",
    "tf.summary.histogram(\"fc1/relu\", fc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'fc2/relu:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc2 = fully_connected_layer(fc1,NODES_H1,NODES_H2)\n",
    "fc2 = activation_function(fc2,\"relu\",\"fc2\")\n",
    "tf.summary.histogram(\"fc2/relu\", fc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'fc2/relu_1:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc3 = fully_connected_layer(fc2,NODES_H2,NODES_H3)\n",
    "fc3 = activation_function(fc3,\"relu\",\"fc3\")\n",
    "tf.summary.histogram(\"fc2/relu\", fc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'output/softmax:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = fully_connected_layer(fc3, NODES_H3, NODES_OUPUT)\n",
    "nah_predicted = activation_function(output, \"softmax\",\"output\")\n",
    "tf.summary.histogram(\"output/softmax\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Función de error\n",
    "Se utiliza la función de error por mínimos cuadrados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=nah_predicted))\n",
    "#loss = tf.reduce_mean(tf.reduce_sum((nah_predicted - y) ** 2))\n",
    "#with tf.name_scope(\"MSE\"):\n",
    "loss = tf.reduce_mean(tf.squared_difference(nah_predicted, y), name=\"loss\")\n",
    "tf.summary.scalar(\"loss\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimiser\n",
    "\n",
    "**NOTAS**\n",
    "> a) En pruebas, al parecer se presenta el problema de [Vanishing Gradient Problem(https://medium.com/@anishsingh20/the-vanishing-gradient-problem-48ae7f501257), la función de error parecía quedarse estancada en un mínimo local. Para contrarrestar esto, se utiliza la función `tf.clip_by_global_norm` que ajusta el gradiente a un valor específico y evitar que rebase un determinado umbral o se haga cero. [Ver liga](https://www.tensorflow.org/versions/r0.12/api_docs/python/train/gradient_clipping)\n",
    "\n",
    "> b) En pruebas, el optimizador para el algoritmo de backpropagation [AdamOptimizer](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer) se queda estancado apenas empieza el entrenamiento (100000 epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/36498127/how-to-effectively-apply-gradient-clipping-in-tensor-flow\n",
    "\n",
    "# Create an optimizer.\n",
    "optimiser = tf.train.AdagradOptimizer(learning_rate=LEARNING_RATE)\n",
    "\n",
    "# Compute gradients\n",
    "gradients, variables = zip(*optimiser.compute_gradients(loss))\n",
    "\n",
    "# For those who would like to understand the idea of gradient clipping(by norm):\n",
    "# Whenever the gradient norm is greater than a particular threshold, \n",
    "# we clip the gradient norm so that it stays within the threshold. \n",
    "# This threshold is sometimes set to 5.\n",
    "# https://stackoverflow.com/questions/36498127/how-to-effectively-apply-gradient-clipping-in-tensor-flow\n",
    "gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "\n",
    "# Apply processed gradients to optimizer.\n",
    "train_op = optimiser.apply_gradients(zip(gradients, variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy\n",
    "Se calcula la precisión de la red neuronal.\n",
    "\n",
    "- [x] Evaluar con lexicon semilla. (para pruebas de visualización de precisión en `TensorBoard`)\n",
    "- [ ] Evaluar con lexicon de evaluación.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Accuracy \n",
    "with tf.name_scope('accuracy'):\n",
    "    with tf.name_scope('correct_prediction'):\n",
    "        # Se compara salida de la red neuronal con el vector objetivo.\n",
    "        correct_prediction = tf.equal(tf.argmax(nah_predicted, 1), tf.argmax(y, 1))\n",
    "    with tf.name_scope('accuracy'):\n",
    "        # Se calcula la precisión.\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float64))\n",
    "    tf.summary.scalar('accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logpath: ./logs/NN_MSE_RELU_Adagrad_H_70_42_70_LR_0.7_\n"
     ]
    }
   ],
   "source": [
    "print(\"logpath:\", LOGPATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Session\n",
    "\n",
    "Para poder realizar el entrenamiento se debe iniciar una sesión para que se puedan ejecutar las operaciones para entrenar y evaluar la red neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Configuración para pasar como argumento a la sesión de TensorFlow.\n",
    "# Es para poder ejecutar el grafo en múltiples hilos.\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=4,\n",
    "                        inter_op_parallelism_threads=1,\n",
    "                        #log_device_placement=True\n",
    "                        )\n",
    "\n",
    "# Se crea la sesión\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "# Se ponen los histogramas y valores de las gráficas en una sola variable.\n",
    "summaryMerged = tf.summary.merge_all()\n",
    "\n",
    "# Escribir a disco el grafo generado y las gráficas para visualizar en TensorBoard.\n",
    "writer = tf.summary.FileWriter(LOGPATH, sess.graph)\n",
    "\n",
    "# Se inicializan los valores de los tensores.\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Ejecutando sesión\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feed_dict(*placeholders, memUsage = False):        \n",
    "    return {X: placeholders[0],\n",
    "            y: placeholders[1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 / 100000 \tLoss: 0.156876711407\n",
      "Epoch: 500 / 100000 \tLoss: 0.156808252481\n",
      "Epoch: 1000 / 100000 \tLoss: 0.156735773783\n",
      "Epoch: 1500 / 100000 \tLoss: 0.156654082022\n",
      "Epoch: 2000 / 100000 \tLoss: 0.156554948536\n",
      "Epoch: 2500 / 100000 \tLoss: 0.156423381244\n",
      "Epoch: 3000 / 100000 \tLoss: 0.156227647132\n",
      "Epoch: 3500 / 100000 \tLoss: 0.155880827931\n",
      "Epoch: 4000 / 100000 \tLoss: 0.155049402068\n",
      "Epoch: 4500 / 100000 \tLoss: 0.153022986973\n",
      "Epoch: 5000 / 100000 \tLoss: 0.152406352436\n",
      "Epoch: 5500 / 100000 \tLoss: 0.152071417381\n",
      "Epoch: 6000 / 100000 \tLoss: 0.152018265191\n",
      "Epoch: 6500 / 100000 \tLoss: 0.151995297928\n",
      "Epoch: 7000 / 100000 \tLoss: 0.151978754137\n",
      "Epoch: 7500 / 100000 \tLoss: 0.151961483508\n",
      "Epoch: 8000 / 100000 \tLoss: 0.15193517347\n",
      "Epoch: 8500 / 100000 \tLoss: 0.151908720394\n",
      "Epoch: 9000 / 100000 \tLoss: 0.15189866054\n",
      "Epoch: 9500 / 100000 \tLoss: 0.151889652512\n",
      "Epoch: 10000 / 100000 \tLoss: 0.151880809451\n",
      "Epoch: 10500 / 100000 \tLoss: 0.151871983672\n",
      "Epoch: 11000 / 100000 \tLoss: 0.151863010083\n",
      "Epoch: 11500 / 100000 \tLoss: 0.151853796577\n",
      "Epoch: 12000 / 100000 \tLoss: 0.151844219951\n",
      "Epoch: 12500 / 100000 \tLoss: 0.151834149742\n",
      "Epoch: 13000 / 100000 \tLoss: 0.151823469894\n",
      "Epoch: 13500 / 100000 \tLoss: 0.151811873882\n",
      "Epoch: 14000 / 100000 \tLoss: 0.151798962296\n",
      "Epoch: 14500 / 100000 \tLoss: 0.151783180714\n",
      "Epoch: 15000 / 100000 \tLoss: 0.151756007836\n",
      "Epoch: 15500 / 100000 \tLoss: 0.151670285622\n",
      "Epoch: 16000 / 100000 \tLoss: 0.151651571809\n",
      "Epoch: 16500 / 100000 \tLoss: 0.151622464689\n",
      "Epoch: 17000 / 100000 \tLoss: 0.151569351888\n",
      "Epoch: 17500 / 100000 \tLoss: 0.151554848381\n",
      "Epoch: 18000 / 100000 \tLoss: 0.151540903914\n",
      "Epoch: 18500 / 100000 \tLoss: 0.151526989296\n",
      "Epoch: 19000 / 100000 \tLoss: 0.15151301233\n",
      "Epoch: 19500 / 100000 \tLoss: 0.151498859288\n",
      "Epoch: 20000 / 100000 \tLoss: 0.151484528725\n",
      "Epoch: 20500 / 100000 \tLoss: 0.151470052708\n",
      "Epoch: 21000 / 100000 \tLoss: 0.151455528852\n",
      "Epoch: 21500 / 100000 \tLoss: 0.151440916738\n",
      "Epoch: 22000 / 100000 \tLoss: 0.15142626217\n",
      "Epoch: 22500 / 100000 \tLoss: 0.15141148677\n",
      "Epoch: 23000 / 100000 \tLoss: 0.151396476673\n",
      "Epoch: 23500 / 100000 \tLoss: 0.151381211684\n",
      "Epoch: 24000 / 100000 \tLoss: 0.151366084934\n",
      "Epoch: 24500 / 100000 \tLoss: 0.151350821553\n",
      "Epoch: 25000 / 100000 \tLoss: 0.151335487388\n",
      "Epoch: 25500 / 100000 \tLoss: 0.151320036086\n",
      "Epoch: 26000 / 100000 \tLoss: 0.151304485391\n",
      "Epoch: 26500 / 100000 \tLoss: 0.151288871314\n",
      "Epoch: 27000 / 100000 \tLoss: 0.15127318117\n",
      "Epoch: 27500 / 100000 \tLoss: 0.151257392976\n",
      "Epoch: 28000 / 100000 \tLoss: 0.151241493683\n",
      "Epoch: 28500 / 100000 \tLoss: 0.151225434523\n",
      "Epoch: 29000 / 100000 \tLoss: 0.151209189331\n",
      "Epoch: 29500 / 100000 \tLoss: 0.151192735366\n",
      "Epoch: 30000 / 100000 \tLoss: 0.151176143216\n",
      "Epoch: 30500 / 100000 \tLoss: 0.151159371055\n",
      "Epoch: 31000 / 100000 \tLoss: 0.151142511278\n",
      "Epoch: 31500 / 100000 \tLoss: 0.151125514593\n",
      "Epoch: 32000 / 100000 \tLoss: 0.151108333924\n",
      "Epoch: 32500 / 100000 \tLoss: 0.151090944471\n",
      "Epoch: 33000 / 100000 \tLoss: 0.151073348243\n",
      "Epoch: 33500 / 100000 \tLoss: 0.151055565775\n",
      "Epoch: 34000 / 100000 \tLoss: 0.151037495753\n",
      "Epoch: 34500 / 100000 \tLoss: 0.15101928259\n",
      "Epoch: 35000 / 100000 \tLoss: 0.151000849134\n",
      "Epoch: 35500 / 100000 \tLoss: 0.150982165002\n",
      "Epoch: 36000 / 100000 \tLoss: 0.150963319725\n",
      "Epoch: 36500 / 100000 \tLoss: 0.150944346149\n",
      "Epoch: 37000 / 100000 \tLoss: 0.150925187904\n",
      "Epoch: 37500 / 100000 \tLoss: 0.150905866034\n",
      "Epoch: 38000 / 100000 \tLoss: 0.150886427889\n",
      "Epoch: 38500 / 100000 \tLoss: 0.150866833565\n",
      "Epoch: 39000 / 100000 \tLoss: 0.150847114224\n",
      "Epoch: 39500 / 100000 \tLoss: 0.150827279178\n",
      "Epoch: 40000 / 100000 \tLoss: 0.150807315373\n",
      "Epoch: 40500 / 100000 \tLoss: 0.150787263055\n",
      "Epoch: 41000 / 100000 \tLoss: 0.150767125676\n",
      "Epoch: 41500 / 100000 \tLoss: 0.150746892559\n",
      "Epoch: 42000 / 100000 \tLoss: 0.150726588581\n",
      "Epoch: 42500 / 100000 \tLoss: 0.15070622502\n",
      "Epoch: 43000 / 100000 \tLoss: 0.150685838222\n",
      "Epoch: 43500 / 100000 \tLoss: 0.150665388404\n",
      "Epoch: 44000 / 100000 \tLoss: 0.150644858494\n",
      "Epoch: 44500 / 100000 \tLoss: 0.150624265783\n",
      "Epoch: 45000 / 100000 \tLoss: 0.150603510726\n",
      "Epoch: 45500 / 100000 \tLoss: 0.150582709485\n",
      "Epoch: 46000 / 100000 \tLoss: 0.150561838691\n",
      "Epoch: 46500 / 100000 \tLoss: 0.15054085658\n",
      "Epoch: 47000 / 100000 \tLoss: 0.15051977753\n",
      "Epoch: 47500 / 100000 \tLoss: 0.150498503299\n",
      "Epoch: 48000 / 100000 \tLoss: 0.150477094806\n",
      "Epoch: 48500 / 100000 \tLoss: 0.150455532669\n",
      "Epoch: 49000 / 100000 \tLoss: 0.150433867336\n",
      "Epoch: 49500 / 100000 \tLoss: 0.150412123552\n",
      "Epoch: 50000 / 100000 \tLoss: 0.150390283888\n",
      "Epoch: 50500 / 100000 \tLoss: 0.150368365075\n",
      "Epoch: 51000 / 100000 \tLoss: 0.150346336736\n",
      "Epoch: 51500 / 100000 \tLoss: 0.150324209681\n",
      "Epoch: 52000 / 100000 \tLoss: 0.150301829469\n",
      "Epoch: 52500 / 100000 \tLoss: 0.150279295902\n",
      "Epoch: 53000 / 100000 \tLoss: 0.150256774552\n",
      "Epoch: 53500 / 100000 \tLoss: 0.150234301062\n",
      "Epoch: 54000 / 100000 \tLoss: 0.150211875807\n",
      "Epoch: 54500 / 100000 \tLoss: 0.150189520248\n",
      "Epoch: 55000 / 100000 \tLoss: 0.150167228451\n",
      "Epoch: 55500 / 100000 \tLoss: 0.150144885474\n",
      "Epoch: 56000 / 100000 \tLoss: 0.150122531436\n",
      "Epoch: 56500 / 100000 \tLoss: 0.150100123114\n",
      "Epoch: 57000 / 100000 \tLoss: 0.150077755222\n",
      "Epoch: 57500 / 100000 \tLoss: 0.150055443887\n",
      "Epoch: 58000 / 100000 \tLoss: 0.150033206604\n",
      "Epoch: 58500 / 100000 \tLoss: 0.15001112093\n",
      "Epoch: 59000 / 100000 \tLoss: 0.149989160804\n",
      "Epoch: 59500 / 100000 \tLoss: 0.14996735781\n",
      "Epoch: 60000 / 100000 \tLoss: 0.149945726993\n",
      "Epoch: 60500 / 100000 \tLoss: 0.149924259486\n",
      "Epoch: 61000 / 100000 \tLoss: 0.14990300134\n",
      "Epoch: 61500 / 100000 \tLoss: 0.149881885635\n",
      "Epoch: 62000 / 100000 \tLoss: 0.149860926561\n",
      "Epoch: 62500 / 100000 \tLoss: 0.149840136099\n",
      "Epoch: 63000 / 100000 \tLoss: 0.149819417998\n",
      "Epoch: 63500 / 100000 \tLoss: 0.149798857983\n",
      "Epoch: 64000 / 100000 \tLoss: 0.149778484746\n",
      "Epoch: 64500 / 100000 \tLoss: 0.149758219066\n",
      "Epoch: 65000 / 100000 \tLoss: 0.149738108693\n",
      "Epoch: 65500 / 100000 \tLoss: 0.149717997581\n",
      "Epoch: 66000 / 100000 \tLoss: 0.149697985779\n",
      "Epoch: 66500 / 100000 \tLoss: 0.149678111815\n",
      "Epoch: 67000 / 100000 \tLoss: 0.14965846021\n",
      "Epoch: 67500 / 100000 \tLoss: 0.149639112678\n",
      "Epoch: 68000 / 100000 \tLoss: 0.14962015571\n",
      "Epoch: 68500 / 100000 \tLoss: 0.149601641713\n",
      "Epoch: 69000 / 100000 \tLoss: 0.149583569907\n",
      "Epoch: 69500 / 100000 \tLoss: 0.149566002792\n",
      "Epoch: 70000 / 100000 \tLoss: 0.149548869135\n",
      "Epoch: 70500 / 100000 \tLoss: 0.14953214821\n",
      "Epoch: 71000 / 100000 \tLoss: 0.149515893479\n",
      "Epoch: 71500 / 100000 \tLoss: 0.149500013146\n",
      "Epoch: 72000 / 100000 \tLoss: 0.14948444816\n",
      "Epoch: 72500 / 100000 \tLoss: 0.149469136822\n",
      "Epoch: 73000 / 100000 \tLoss: 0.149454104803\n",
      "Epoch: 73500 / 100000 \tLoss: 0.149439302562\n",
      "Epoch: 74000 / 100000 \tLoss: 0.149424712835\n",
      "Epoch: 74500 / 100000 \tLoss: 0.149410414994\n",
      "Epoch: 75000 / 100000 \tLoss: 0.149396308407\n",
      "Epoch: 75500 / 100000 \tLoss: 0.149382419761\n",
      "Epoch: 76000 / 100000 \tLoss: 0.149368721706\n",
      "Epoch: 76500 / 100000 \tLoss: 0.149355306171\n",
      "Epoch: 77000 / 100000 \tLoss: 0.149342673955\n",
      "Epoch: 77500 / 100000 \tLoss: 0.149331433727\n",
      "Epoch: 78000 / 100000 \tLoss: 0.149321039525\n",
      "Epoch: 78500 / 100000 \tLoss: 0.149310349191\n",
      "Epoch: 79000 / 100000 \tLoss: 0.149299587689\n",
      "Epoch: 79500 / 100000 \tLoss: 0.149288569889\n",
      "Epoch: 80000 / 100000 \tLoss: 0.14927793462\n",
      "Epoch: 80500 / 100000 \tLoss: 0.149267153293\n",
      "Epoch: 81000 / 100000 \tLoss: 0.149256512562\n"
     ]
    }
   ],
   "source": [
    "for i in range(EPOCHS):\n",
    "    \n",
    "    # Se corre la sesión y se pasan como argumentos la función de error (loss),\n",
    "    # el optimizador de backpropagation (train_op) y los histogramas (summaryMerged)\n",
    "    if i % 5 == 0:\n",
    "        _loss, _, sumOut = sess.run([loss, train_op, summaryMerged],\n",
    "                                    feed_dict=feed_dict(es_vectores,na_vectores))\n",
    "        # Actualiza los histogramas.\n",
    "        writer.add_summary(sumOut, i)\n",
    "    \n",
    "    # Muestra el valor del error cada 500 pasos de entrenamiento.\n",
    "    if (i % 500) == 0:\n",
    "        print(\"Epoch:\",i,\"/\",EPOCHS, \"\\tLoss:\", _loss)\n",
    "\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
