{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://codelabs.developers.google.com/codelabs/cloud-tensorflow-mnist/#13\n",
    "\n",
    "import tensorflow as tf\n",
    "import utils\n",
    "__author__ = \"Olivares Castillo José Luis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset everything to rerun in jupyter\n",
    "tf.reset_default_graph()\n",
    "print(\"TensorFlow v{}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semilla para reproducibilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargar vectores desde archivos.\n",
    "Leer archivos node2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es, na = utils.load_node2vec()\n",
    "print(\"es:\",es.shape,\"\\tna:\",na.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buscar palabra por índice\n",
    "print(es.iloc[358][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se buscan los índices de los lexicones semilla dentro de los dataframes para poder acceder a sus representaciones vectoriales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_es, index_na = utils.get_seed_index(es,na)\n",
    "print(\"index_es:\",index_es.__len__(),\"index_na:\",index_na.__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se obtienen los vectores de los lexicones semilla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "es_vectores = utils.get_vectors(es,index_es)\n",
    "na_vectores = utils.get_vectors(na,index_na)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.46\n",
    "NODES_INPUT = es_vectores[0].size\n",
    "NODES_H1 = 300\n",
    "#NODES_H2 = 1\n",
    "#NODES_H3 = 1\n",
    "NODES_OUPUT = na_vectores[0].size\n",
    "EPOCHS = 100000\n",
    "\n",
    "# Inicializar pesos con método xavier_init\n",
    "XAVIER_INIT = False\n",
    "\n",
    "# Ruta donde se guarda el grafo para visualizar en TensorBoard.\n",
    "LOGPATH = utils.make_hparam_string(\"80ACC_Adagrad\", \"H\", NODES_H1, \"LR\", LEARNING_RATE)\n",
    "\n",
    "# Ruta para guardar el modelo generado.\n",
    "SAVE_PATH = \"./models/Adagrad_H_305_LR_0.433.ckpt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Placeholders\n",
    "\n",
    "Tensores donde estarán los vectores de entrada y salida.\n",
    "\n",
    "* X: Vectores de español.\n",
    "* y: Vectores de náhuatl.\n",
    "\n",
    "`tf.name_scope` se utiliza para mostrar las entradas del grafo computacional en `TensorBoard`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('input'):\n",
    "    # El valor None indica que se puede modificar la dimensión de los tensores\n",
    "    # por si se usan todos los vectores o batches.\n",
    "    X = tf.placeholder(shape=[None, NODES_INPUT],dtype=tf.float64, name='input_es')\n",
    "    y = tf.placeholder(shape=[None, NODES_OUPUT],dtype=tf.float64, name='target_na')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Función para crear las capas de la red.\n",
    "    \n",
    "Arguments:\n",
    "* input {Tensor} -- Tensor de entrada a la capa.\n",
    "* size_in {int}, size_out {int} -- Dimensiones de entrada y salida de la capa.\n",
    "* name {str} -- Nombre de la capa. Default: fc.\n",
    "Keyword Arguments:\n",
    "* xavier_init {bool} -- Inicializar pesos empleando el método Xavier.\n",
    "* stddev {float} -- Desviación estándar con la que se inicializan los pesos de la capa. (default: {0})\n",
    "* dtype {function} -- Floating-point representation. (default: {tf.float64})\n",
    "\n",
    "Returns:\n",
    "* Tensor -- Salida de la capa: (input * Weights) + bias\n",
    "\n",
    "# Inicialización de pesos.\n",
    "Si la bandera `XAVIER_INIT` es `True` se emplea el método Xavier, en caso contrario los pesos se inicializan con valores siguiendo una distribución normal.\n",
    "## Xavier Initialization\n",
    ">This initializer is designed to keep the scale of the gradients roughly the same in all layers. In uniform distribution this ends up being the \n",
    "range: x = sqrt(6. / (in + out)); [-x, x] and for normal distribution a standard deviation of sqrt(2. / (in + out)) is used.\n",
    "[Xavier Glorot and Yoshua Bengio (2010)](http://www.jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf)\n",
    "\n",
    ">Why’s Xavier initialization important?\n",
    "In short, it helps signals reach deep into the network.\n",
    "* If the weights in a network start too small, then the signal shrinks as it passes through each layer until it’s too tiny to be useful.\n",
    "* If the weights in a network start too large, then the signal grows as it passes through each layer until it’s too massive to be useful.\n",
    "\n",
    ">Xavier initialization makes sure the weights are ‘just right’, keeping the signal in a reasonable range of values through many layers.\n",
    "[Ver liga](http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fully_connected_layer(input, size_in, size_out, name,xavier_init=True, stddev=0.1, dtype=tf.float64):\n",
    "    with tf.name_scope(name):\n",
    "        # Inicializar pesos\n",
    "        if xavier_init:\n",
    "            W = tf.get_variable(name=\"W_\" + name, shape=[size_in, size_out], dtype=dtype,\n",
    "                                initializer=tf.contrib.layers.xavier_initializer(dtype=dtype),\n",
    "                                use_resource=True)\n",
    "        else:\n",
    "            W = tf.Variable(tf.truncated_normal([size_in, size_out], stddev=stddev,dtype=dtype), name=\"W\")\n",
    "        # Bias\n",
    "        b = tf.Variable(tf.constant(0.1, shape=[size_out], dtype=dtype), name=\"b\")\n",
    "\n",
    "        # h(x) = (input * weights) + bias\n",
    "        output = tf.nn.xw_plus_b(input, W, b)\n",
    "        \n",
    "        # visualizarlos pesos en TensorBoard.\n",
    "        tf.summary.histogram(\"weights\", W)\n",
    "        tf.summary.histogram(\"xw_plus_b\", output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activación de capas.\n",
    "Esta función aplica la activación a la capa neuronal.\n",
    "\n",
    "Arguments:\n",
    "* layer {Tensor} -- Capa a activar.\n",
    "* act {tf.function} -- Función de activación (default: {tf.nn.relu}).\n",
    "* name {str} -- Nombre para visualización de activación en TensorBoard.\n",
    "\n",
    "Keyword Arguments:\n",
    "* alpha {tf.constant} -- Constante que se usa como argumento para leaky_relu (default: {tf.constant(0.2)})\n",
    "* dtype {tf.function} -- Floating-point representation. (default: {tf.float64})\n",
    "\n",
    "Returns:\n",
    "* Tensor -- Capa con función de activación aplicada.\n",
    "\n",
    "**NOTA:**\n",
    ">3.4 Why do we use a leaky ReLU and not a ReLU as an activation function?\n",
    "We want gradients to flow while we backpropagate through the network. \n",
    "We stack many layers in a system in which there are some neurons \n",
    "whose value drop to zero or become negative. Using a ReLU as an activation \n",
    "function clips the negative values to zero and in the backward pass, \n",
    "the gradients do not flow through those neurons where the values become zero. \n",
    "Because of this the weights do not get updated, and the network stops learning \n",
    "for those values. So using ReLU is not always a good idea. However, we encourage \n",
    "you to change the activation function to ReLU and see the difference.\n",
    "[See link](https://www.learnopencv.com/understanding-autoencoders-using-tensorflow-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def activation_function(layer, act, name, alpha=tf.constant(0.2, dtype=tf.float64)):\n",
    "    # Funciones de activación.\n",
    "    if act == \"leaky_relu\":\n",
    "        return tf.nn.leaky_relu(layer, alpha, name=name)\n",
    "    elif act == \"softmax\":\n",
    "        return tf.nn.softmax(layer, name=name)\n",
    "    elif act == \"sigmoid\":\n",
    "        return tf.nn.sigmoid(layer, name=name)\n",
    "    elif act == \"tanh\":\n",
    "        return tf.nn.tanh(layer, name=name)\n",
    "    return tf.nn.relu(layer, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se definen las capas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se calcula la salida de la capa.\n",
    "fc1 = fully_connected_layer(X, NODES_INPUT, NODES_H1, \"fc1\",xavier_init=XAVIER_INIT)\n",
    "\n",
    "# Activación de la capa.\n",
    "fc1 = activation_function(fc1, \"relu\", \"fc1\")\n",
    "\n",
    "# Se añade histograma de activación de la capa para visualizar en TensorBoard.\n",
    "tf.summary.histogram(\"fc1/relu\", fc1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fc2 = fully_connected_layer(fc1, NODES_H1, NODES_H2, \"fc2\")\n",
    "fc2 = activation_function(fc2, \"relu\", \"fc2\")\n",
    "tf.summary.histogram(\"fc2/relu\", fc2)\n",
    "#fc2 = tf.nn.dropout(fc2, pkeep)\n",
    "# In[ ]:\n",
    "#fc2 = tf.nn.dropout(fc2, pkeep)\n",
    "fc3 = fully_connected_layer(fc2, NODES_H2, NODES_H3, \"fc3\")\n",
    "fc3 = activation_function(fc3, \"relu\", \"fc3\")\n",
    "tf.summary.histogram(\"fc2/relu\", fc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = fully_connected_layer(fc1, NODES_H1, NODES_OUPUT, \"output\",xavier_init=XAVIER_INIT)\n",
    "nah_predicted = activation_function(output, \"sigmoid\", \"output\")\n",
    "tf.summary.histogram(\"output/sigmoid\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Función de error\n",
    "Se utiliza la función de error por mínimos cuadrados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=nah_predicted))\n",
    "#loss = tf.reduce_mean(tf.reduce_sum((nah_predicted - y) ** 2))\n",
    "#with tf.name_scope(\"MSE\"):\n",
    "loss = tf.reduce_mean(tf.squared_difference(nah_predicted, y), name=\"loss\")\n",
    "tf.summary.scalar(\"loss\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimiser\n",
    "\n",
    "**NOTAS**\n",
    "> a) En pruebas, al parecer se presenta el problema de [Vanishing Gradient Problem(https://medium.com/@anishsingh20/the-vanishing-gradient-problem-48ae7f501257), la función de error parecía quedarse estancada en un mínimo local. Para contrarrestar esto, se utiliza la función `tf.clip_by_global_norm` que ajusta el gradiente a un valor específico y evitar que rebase un determinado umbral o se haga cero. [Ver liga](https://www.tensorflow.org/versions/r0.12/api_docs/python/train/gradient_clipping)\n",
    "\n",
    "> b) En pruebas, el optimizador para el algoritmo de backpropagation [AdamOptimizer](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer) se queda estancado apenas empieza el entrenamiento (100000 epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/36498127/how-to-effectively-apply-gradient-clipping-in-tensor-flow\n",
    "\n",
    "# Create an optimizer.\n",
    "optimiser = tf.train.AdagradOptimizer(learning_rate=LEARNING_RATE)\n",
    "\n",
    "# Compute gradients\n",
    "gradients, variables = zip(*optimiser.compute_gradients(loss))\n",
    "\n",
    "# For those who would like to understand the idea of gradient clipping(by norm):\n",
    "# Whenever the gradient norm is greater than a particular threshold, \n",
    "# we clip the gradient norm so that it stays within the threshold. \n",
    "# This threshold is sometimes set to 5.\n",
    "# https://stackoverflow.com/questions/36498127/how-to-effectively-apply-gradient-clipping-in-tensor-flow\n",
    "gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "\n",
    "# Apply processed gradients to optimizer.\n",
    "train_op = optimiser.apply_gradients(zip(gradients, variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy\n",
    "Se calcula la precisión de la red neuronal.\n",
    "\n",
    "- [x] Evaluar con lexicon semilla. (para pruebas de visualización de precisión en `TensorBoard`)\n",
    "- [ ] Evaluar con lexicon de evaluación.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Accuracy \n",
    "with tf.name_scope('accuracy'):\n",
    "    with tf.name_scope('correct_prediction'):\n",
    "        # Se compara salida de la red neuronal con el vector objetivo.\n",
    "        correct_prediction = tf.equal(tf.argmax(nah_predicted, 1), tf.argmax(y, 1))\n",
    "    with tf.name_scope('accuracy'):\n",
    "        # Se calcula la precisión.\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float64))\n",
    "    tf.summary.scalar('accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"logpath:\", LOGPATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Session\n",
    "\n",
    "Para poder realizar el entrenamiento se debe iniciar una sesión para que se puedan ejecutar las operaciones para entrenar y evaluar la red neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Configuración para pasar como argumento a la sesión de TensorFlow.\n",
    "# Es para poder ejecutar el grafo en múltiples hilos.\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=4,\n",
    "                        inter_op_parallelism_threads=1,\n",
    "                        #log_device_placement=True\n",
    "                        )\n",
    "\n",
    "# Se crea la sesión\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "# Se ponen los histogramas y valores de las gráficas en una sola variable.\n",
    "summaryMerged = tf.summary.merge_all()\n",
    "\n",
    "# Escribir a disco el grafo generado y las gráficas para visualizar en TensorBoard.\n",
    "writer = tf.summary.FileWriter(LOGPATH, sess.graph)\n",
    "\n",
    "# Se inicializan los valores de los tensores.\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Add ops to save and restore all the variables.\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Ejecutando sesión\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feed_dict(*placeholders):\n",
    "    return {X: placeholders[0],\n",
    "            y: placeholders[1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(EPOCHS):\n",
    "    # learning rate decay\n",
    "    # https://github.com/martin-gorner/tensorflow-mnist-tutorial/blob/master/mnist_2.1_five_layers_relu_lrdecay.py\n",
    "    # Se corre la sesión y se pasan como argumentos la función de error (loss),\n",
    "    # el optimizador de backpropagation (train_op) y los histogramas (summaryMerged)\n",
    "\n",
    "    _loss, _, sumOut = sess.run([loss, train_op, summaryMerged],feed_dict={X: es_vectores,y: na_vectores})\n",
    "    # Actualiza los histogramas.\n",
    "    writer.add_summary(sumOut, i)\n",
    "\n",
    "    # Muestra el valor del error cada 500 pasos de entrenamiento.\n",
    "    if (i % 500) == 0:\n",
    "        train_accuracy = accuracy.eval(session=sess, feed_dict={X: es_vectores,y: na_vectores})\n",
    "        print(\"Epoch:\", i, \"/\", EPOCHS, \"\\tLoss:\",_loss, \"\\tAccuracy:\", train_accuracy)\n",
    "        \n",
    "SAVE_PATH = \"./models/Adagrad_H_305_LR_0.433.ckpt\"\n",
    "save_model = saver.save(sess, SAVE_PATH)\n",
    "print(\"Model saved in file: %s\", save_path)\n",
    "    #print(\"\\nAccuracy:\", accuracy.eval(feed_dict=feed_dict(es_vectores, na_vectores)))\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
