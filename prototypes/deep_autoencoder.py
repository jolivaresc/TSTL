# -*- coding: utf-8 -*-
"""autoencoder.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eEw3YSM-XGLjUhrkrWKYQGxDAXaqlJdj
"""

import tensorflow as tf
import pandas as pd
import numpy as np
from sklearn.neural_network import BernoulliRBM
from google.colab import files

__author__ = "Olivares Castillo José Luis"

print("TensorFlow",tf.__version__)
if tf.test.gpu_device_name():
  print("GPU disponible")

LOG_DIR = '/tmp/model1'
get_ipython().system_raw(
    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'
    .format(LOG_DIR)
)

! curl http://localhost:6006

! wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip > /dev/null 2>&1
! unzip ngrok-stable-linux-amd64.zip > /dev/null 2>&1

get_ipython().system_raw('./ngrok http 6006 &')

! curl -s http://localhost:4040/api/tunnels | python3 -c \
    "import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])"

# Install
! npm install -g localtunnel

# Tunnel port 6006 (TensorBoard assumed running)
get_ipython().system_raw('lt --port 6006 >> url.txt 2>&1 &')

# Get url
! cat url.txt

"""# Cargar archivos necesarios
## Lexicon de entrenamiento
"""

##############################
# Cargar archivos necesarios #
##############################
# Cargar lexicon semilla de entrenamiento
# `lexiconfile` es de type dict.
train_set = files.upload()

# Se convierte el archivo a una lista
train_set = train_set['OPUS_en_it_europarl_train_5K.txt'].decode("utf-8").split("\n")

#tuple(lexicon[0].split())

train_set.__len__()

"""## Lexicon de evaluación"""

'''test_set = files.upload()
test_set = test_set['evaluationset'].decode("utf-8").split("\n")'''

"""## Función para crear dataframes de lexicones"""

def create_lexicon_dataframe(lexicon):
  # Separar cada elemento de lista en una tupla
  lexicon_ = list()
  for i in lexicon:
    lexicon_.append(tuple(i.split()))

  # Crear dataframe con los pares traducción
  lexicon_df = pd.DataFrame.from_records(lexicon_, columns=["esp", "nah"])
  del lexicon_
  return lexicon_df

"""### Crear dataframes de lexicones"""

train_set = create_lexicon_dataframe(train_set)
#test_set = create_lexicon_dataframe(test_set)
train_set.shape#,test_set.shape

pares_eval = list(zip(e, n))

train_set = pd.DataFrame.from_records(pares_eval, columns=["esp", "nah"])

"""# Vectores N2V/W2V"""

# Cargar vectores náhuatl
na_n2v = files.upload()

na_w2v3 = files.upload()
na_w2v3=na_w2v3['na.n2v.seed'].decode("utf-8").split("\n")

es_w2v3 = files.upload()
es_w2v3 = es_w2v3['es.n2v.seed'].decode("utf-8").split("\n")

# Cargar vectores español
es_n2v = files.upload()

es_n2v.keys()

# Convertir archivos a una lista
es_n2v = es_n2v['NEW-EN'].decode("utf-8").split("\n")
na_n2v = na_n2v['NEW-IT'].decode("utf-8").split("\n")

def create_vectors_dataframe(df):
  # Separar cada elemento de la lista en una tupla
  df_tmp = list()
  for i in df:
      if i == 0:
          pass
      else:
          df_tmp.append(tuple(i.split()))

  # Eliminar el primer elemento, no se utiliza
  #df_tmp.pop(0)
  return pd.DataFrame.from_records(df_tmp)

# Crear dataframes con los vectores
es_df = create_vectors_dataframe(es_n2v)
na_df = create_vectors_dataframe(na_n2v)

es_df.head()

na_df.head()

es_df.shape,na_df.shape

# obtener listas de semillas de español y náhuatl de entrenamiento
semillas_esp = list(train_set["esp"].values)
semillas_nah = list(train_set["nah"].values)

# Obtener los índices que tienen las semillas de entrenamiento
# dentro del dataframe de vectores
index_esp = [int(es_df[es_df[0] == palabra].index[0])
             for palabra in semillas_esp]
index_nah = [int(na_df[na_df[0] == palabra].index[0])
             for palabra in semillas_nah]

print(len(index_esp), len(index_nah))

# obtener listas de semillas de español y náhuatl de evaluación
#semillas_esp_test = list(set(test_set["esp"].values))
#semillas_nah_test = list(set(test_set["nah"].values))

def get_vectors(dataframe, index, format=np.float64):
    """
    Retorna los vectores dentro del dataframe.
    
    Args:
        dataframe (Pandas.dataframe): Contiene las palabras y su representación vectorial.
        index (list): Contiene los índices que se necesitan del dataframe.
        format (numpy format): Tipo flotante. Default float64.
    
    Returns:
        Numpy array: Matriz con representaciones vectoriales.
    """
    
    return np.asarray([dataframe.iloc[_].loc[1::].values for _ in index]).astype(format)

# Obtener representaciones vectoriales de las semillas.
es_vectores = get_vectors(es_df, index_esp)
na_vectores = get_vectors(na_df, index_nah)
len(es_vectores)

"""# Los dataframes solo contienen los vectores de entrenamiento, ya no es necesario buscarlo por índices"""

na_dummy = na_df.drop(na_df.columns[0], axis=1)
na_vectores = na_dummy.values.astype(np.float64)
es_dummy = es_df.drop(es_df.columns[0], axis=1)
es_vectores = es_dummy.values.astype(np.float64)

es_vectores=es_df.as_matrix()
na_vectores=es_df.as_matrix()

es_vectores.shape,na_vectores.shape

#index_dummy=[np.random.randint(24) for _ in range(150)]
es_dummy = es_vectores
na_dummy = na_vectores
for i in range(es_dummy.shape[0]):
    es_vectores=np.vstack((es_vectores,es_dummy[i][::-1]))
    na_vectores=np.vstack((na_vectores,na_dummy[i][::-1]))

def mean_center_embeddingwise(matrix):
    xp = (matrix)
    avg = np.mean(matrix, axis=0)
    return matrix - avg

es_vectores=mean_center_embeddingwise(es_vectores)
na_vectores=mean_center_embeddingwise(na_vectores)

np.min(es_vectores)

es_df[es_df[0]=="cebolla"]

es_vect = es_vectores
na_vect = na_vectores

def next_batch(x,y, step, batch_size):
    """Función para obtener batches de un conjunto de datos
    
    Arguments:
        x {numpyarray} -- Conjunto de datos (inputs).
        y {numpyarray} -- Conjunto de datos (targets).
        step {int} -- Batches.
        batch_size {int} -- Tamaño del batch.
    
    Returns:
        numpyarray -- Subconjunto de tamaño batch_size.
    """

    return x[batch_size * step:batch_size * step + batch_size],y[batch_size * step:batch_size * step + batch_size]

"""%%time
rbm = BernoulliRBM(n_components=500, learning_rate=.0001, random_state=0, verbose=True,n_iter=15)
rbm.fit(es_vectores)
W1_ = rbm.components_.T
rbm = BernoulliRBM(n_components=250, learning_rate=.0001, random_state=0, verbose=True,n_iter=15)
rbm.fit(W1_)
W2_= rbm.components_.T
rbm = BernoulliRBM(n_components=100, learning_rate=.0001, random_state=0, verbose=True,n_iter=15)
rbm.fit(W2_)
W3_ = rbm.components_.T
"""

es_vectores.shape

"""https://ikhlestov.github.io/posts/rbm-based-autoencoders-with-tensorflow/
https://gist.github.com/blackecho/db85fab069bd2d6fb3e7

# Entrenamiento
"""

tf.reset_default_graph()
#tf.set_random_seed(42)
print("TensorFlow v{}".format(tf.__version__))
print(tf.test.gpu_device_name())

LEARNING_RATE = 1
EPOCHS = 500
# Dimensión de vectores de entrada (número de neuronas en capa de entrada).
NODES_INPUT = es_vectores.shape[1]

# Número de neuronas en capas ocultas.
NODES_H1 = 350   #70 - 20 - 15
NODES_H2 = 250  # 42 - 20
NODES_H3 = 100  # 42 - 20
NODES_H4 = 50  # 42 - 20
NODES_H5 = 950


NODES_OUTPUT = na_vectores.shape[1]
#XAVIER_INIT = True
INIT = {"HE":tf.keras.initializers.he_uniform(seed=42),
        "XAVIER":tf.contrib.layers.xavier_initializer(dtype=tf.float64,seed=42),
        "TANH":tf.truncated_normal_initializer(stddev=0.1,seed=42),
        "w1":tf.truncated_normal_initializer(stddev=0.07688522844907134,mean=0.00019678121094947982,seed=42),
        "w2":tf.truncated_normal_initializer(stddev=0.08457032845641706,mean=0.00036622167006112975,seed=42)
       }

DROP = 1

model = "model2250"


with tf.name_scope('input'):
    # El valor None indica que se puede modificar la dimensión de los tensores
    # por si se usan todos los vectores o batches.
    X = tf.placeholder(shape=[None, NODES_INPUT],dtype=tf.float64, name='input_es')
    y = tf.placeholder(shape=[None, NODES_OUTPUT],dtype=tf.float64, name='target_na')


kprob = tf.placeholder(tf.float64,name='dropout_prob')


def activation_function(layer, act, name, alpha=tf.constant(0.001, dtype=tf.float64)):
    if act.__eq__("leaky_relu"):
        return tf.nn.leaky_relu(layer, alpha, name=name)
    elif act.__eq__("softmax"):
        return tf.nn.softmax(layer, name=name)
    elif act.__eq__("sigmoid"):
        return tf.nn.sigmoid(layer, name=name)
    elif act.__eq__("tanh"):
        return tf.nn.tanh(layer, name=name)
    elif act.__eq__("elu"):
        return tf.nn.elu(layer,name=name)
    elif act.__eq__("selu"):
        return tf.nn.selu(layer,name=name)
    return tf.nn.relu(layer, name=name)


# Se definen las capas.


'''
fc1 = tf.layers.dense(X,NODES_H1,activation=tf.nn.sigmoid,
                      kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.1),
                      kernel_initializer=tf.contrib.layers.xavier_initializer(dtype=tf.float64))
'''

w1 = tf.get_variable(name="W1", dtype=tf.float64,
                                shape=[NODES_INPUT, NODES_H1],
                                initializer=tf.truncated_normal_initializer(stddev=0.1,seed=41),
                                #initializer=W1_,
                                use_resource=True,
                                regularizer = tf.contrib.layers.l2_regularizer(scale=0.1)
                    )
b1 = tf.Variable(tf.zeros(shape=[NODES_H1], dtype=tf.float64), name="b1")
fc1 = activation_function(tf.nn.xw_plus_b(X,w1,b1), "elu", "fc1")
fc1 = tf.nn.dropout(fc1,kprob,seed=42)



w2 = tf.get_variable(name="W2", dtype=tf.float64,
                                shape=[NODES_H1, NODES_H2],
                                initializer=tf.truncated_normal_initializer(stddev=0.1,seed=42),
                                #initializer=W3_,
                                use_resource=True,
                                regularizer = tf.contrib.layers.l2_regularizer(scale=0.1)
                    )
b2 = tf.Variable(tf.zeros(shape=[NODES_H2], dtype=tf.float64), name="b2")
fc2 = activation_function(tf.nn.xw_plus_b(fc1,w2,b2), "elu", "fc2")


w3 = tf.get_variable(name="W3", dtype=tf.float64,
                                shape=[NODES_H2, NODES_H3],
                                initializer=tf.truncated_normal_initializer(stddev=0.1,seed=43),
                                #initializer=W3_,
                                use_resource=True,
                                regularizer = tf.contrib.layers.l2_regularizer(scale=0.1)
                    )
b3 = tf.Variable(tf.zeros(shape=[NODES_H3], dtype=tf.float64), name="b3")
fc3 = activation_function(tf.nn.xw_plus_b(fc2,w3,b3), "elu", "fc3")


w4 = tf.get_variable(name="W4", dtype=tf.float64,
                                shape=[NODES_H3, NODES_H4],
                                initializer=tf.truncated_normal_initializer(stddev=0.1,seed=43),
                                #initializer=W3_,
                                use_resource=True,
                                regularizer = tf.contrib.layers.l2_regularizer(scale=0.1)
                    )
b4 = tf.Variable(tf.zeros(shape=[NODES_H4], dtype=tf.float64), name="b4")
fc4 = tf.nn.xw_plus_b(fc3,w4,b4)
#fc4 = activation_function(tf.nn.xw_plus_b(fc2,w3,b3), "elu", "fc4")

w5 = tf.transpose(w4)
b5 = tf.Variable(tf.zeros(shape=[NODES_H3],dtype=tf.float64),name="b5")
fc5 = activation_function(tf.nn.xw_plus_b(fc4,w5,b5),"elu","fc5")

w6 = tf.transpose(w3)
b6 = tf.Variable(tf.zeros(shape=[NODES_H2],dtype=tf.float64),name="b6")
fc6 = activation_function(tf.nn.xw_plus_b(fc5,w6,b6),"elu","fc5")

w7 = tf.transpose(w2)
b7 = tf.Variable(tf.zeros(shape=[NODES_H1],dtype=tf.float64),name="b7")
fc7= activation_function(tf.nn.xw_plus_b(fc6,w7,b7),"elu","fc7")


w8 = tf.transpose(w1)
b8 = tf.Variable(tf.zeros(shape=[NODES_OUTPUT],dtype=tf.float64),name="b8")
nah_predicted = activation_function(tf.nn.xw_plus_b(fc7,w8,b8),"tanh","nah_predicted")




'''W_na = tf.get_variable(name="W_na",shape=[NODES_H2,NODES_OUTPUT],dtype=tf.float64,
                                initializer=INIT["w2"],
                                use_resource=True,
                                regularizer=tf.contrib.layers.l2_regularizer(scale=0.1)
                                )
b_na = tf.Variable(tf.zeros(shape=[NODES_OUTPUT],dtype=tf.float64),name="b_na")
#nah_predicted=tf.nn.xw_plus_b(fc1,W_na,b_na)
nah_predicted = activation_function(tf.nn.xw_plus_b(fc1,W_na,b_na),"sigmoid","nah_predicted")'''



'''reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)
reg_constant = 0.01  # Choose an appropriate one'''

#loss = tf.reduce_mean(tf.losses.mean_squared_error(nah_predicted,y), name="loss")
loss = tf.reduce_mean(tf.squared_difference(nah_predicted, y), name="loss")

#regularizer = tf.nn.l2_loss(W1) + tf.nn.l2_loss(W_na)

#loss = loss + sum(reg_losses)
#loss = tf.reduce_mean(loss+0.01*regularizer)
tf.summary.scalar("loss", loss)


#optimiser = tf.train.MomentumOptimizer(learning_rate=LEARNING_RATE, momentum=0.7)
optimiser = tf.train.AdagradOptimizer(learning_rate=LEARNING_RATE)
#train_op = tf.train.AdagradOptimizer(learning_rate=LEARNING_RATE).minimize(loss)


# Compute gradients
gradients, variables = zip(*optimiser.compute_gradients(loss))

gradients, _ = tf.clip_by_global_norm(gradients, 5.0)

# Apply processed gradients to optimizer.
train_op = optimiser.apply_gradients(zip(gradients, variables))


# Accuracy 
with tf.name_scope('accuracy'):
    with tf.name_scope('correct_prediction'):
        # Se compara salida de la red neuronal con el vector objetivo.
        correct_prediction = tf.equal(tf.argmax(nah_predicted, 1), tf.argmax(y, 1))
    with tf.name_scope('accuracy'):
        # Se calcula la precisión.
        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float64))
    tf.summary.scalar('accuracy', accuracy)


LOGPATH = "logs/model"
print("logpath:", LOGPATH)


# Se crea la sesión
config = tf.ConfigProto(log_device_placement=True)
sess =  tf.Session(config=config)

# Se ponen los histogramas y valores de las gráficas en una sola variable.
summaryMerged = tf.summary.merge_all()

# Escribir a disco el grafo generado y las gráficas para visualizar en TensorBoard.
writer = tf.summary.FileWriter(LOGPATH, sess.graph)

# Se inicializan los valores de los tensores.
init = tf.global_variables_initializer()

# Add ops to save and restore all the variables.
saver = tf.train.Saver()

# Ejecutando sesión
sess.run(init)


for j in range(EPOCHS):
    
    for i in range(0,20):
      es_batch, na_batch = next_batch(es_vectores,na_vectores, i, 250)
      _loss, _, sumOut = sess.run([loss, train_op, summaryMerged],
                                feed_dict={X: es_batch, y: na_batch,kprob:DROP})
    
    writer.add_summary(sumOut, j)
    
    if (j % 50) == 0:
        #train_accuracy = accuracy.eval(session=sess, feed_dict={X: es_vectores, y: es_vectores,kprob:DROP})
        print("Epoch:", j, "/", EPOCHS, "\tLoss:",_loss)#, "\tAccuracy:", train_accuracy)
        
        
        

SAVE_PATH = "./"+model+".ckpt"
print("save path",SAVE_PATH)
save_model = saver.save(sess, SAVE_PATH)
print("Model saved in file: %s", SAVE_PATH)
writer.close()

#ww=sess.run(W1)
! uname -a

np.mean(ww,dtype=np.float64),np.std(ww,dtype=np.float64)

np.mean(ww,dtype=np.float64),np.std(ww,dtype=np.float64)

np.mean(ww2,dtype=np.float64),np.std(ww2,dtype=np.float64)

"""# Descargar modelo generado"""

files.download("/content/checkpoint")

files.download("/content/"+model+".ckpt.meta")

files.download("/content/"+model+".ckpt.index")

files.download("/content/"+model+".ckpt.data-00000-of-00001")

es_vectores.shape

