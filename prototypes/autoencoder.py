# -*- coding: utf-8 -*-
"""autoencoder.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eEw3YSM-XGLjUhrkrWKYQGxDAXaqlJdj
"""

import tensorflow as tf
import pandas as pd
import numpy as np
from google.colab import files

__author__ = "Olivares Castillo José Luis"

print(tf.__version__, tf.test.gpu_device_name())

"""# Cargar archivos necesarios
## Lexicon de entrenamiento
"""

##############################
# Cargar archivos necesarios #
##############################
# Cargar lexicon semilla de entrenamiento
# `lexiconfile` es de type dict.
train_set = files.upload()

# Se convierte el archivo a una lista
train_set = train_set['newlexiconb.lst'].decode("utf-8").split("\n")

#tuple(lexicon[0].split())

"""## Lexicon de evaluación"""

'''test_set = files.upload()
test_set = test_set['evaluationset'].decode("utf-8").split("\n")'''

"""## Función para crear dataframes de lexicones"""

def create_lexicon_dataframe(lexicon):
  # Separar cada elemento de lista en una tupla
  lexicon_ = list()
  for i in lexicon:
    lexicon_.append(tuple(i.split()))

  # Crear dataframe con los pares traducción
  lexicon_df = pd.DataFrame.from_records(lexicon_, columns=["esp", "nah"])
  del lexicon_
  return lexicon_df

"""### Crear dataframes de lexicones"""

train_set = create_lexicon_dataframe(train_set)
#test_set = create_lexicon_dataframe(test_set)
train_set.shape#,test_set.shape

# Cargar vectores náhuatl
na_n2v = files.upload()

# Cargar vectores español
es_n2v = files.upload()

# Convertir archivos a una lista
es_n2v = es_n2v['es.node2vec.embeddings'].decode("utf-8").split("\n")
na_n2v = na_n2v['na.node2vec.embeddings'].decode("utf-8").split("\n")

def create_vectors_dataframe(df):
  # Separar cada elemento de la lista en una tupla
  df_tmp = list()
  for i in df:
      if i == 0:
          pass
      else:
          df_tmp.append(tuple(i.split()))

  # Eliminar el primer elemento, no se utiliza
  df_tmp.pop(0)
  return pd.DataFrame.from_records(df_tmp)

# Crear dataframes con los vectores
es_df = create_vectors_dataframe(es_n2v)
na_df = create_vectors_dataframe(na_n2v)

es_df.head()

na_df.head()

# obtener listas de semillas de español y náhuatl de entrenamiento
semillas_esp = list(train_set["esp"].values)
#semillas_nah = list(train_set["nah"].values)

# Obtener los índices que tienen las semillas de entrenamiento
# dentro del dataframe de vectores
index_esp = [int(es_df[es_df[0] == palabra].index[0])
             for palabra in semillas_esp]
index_nah = [int(na_df[na_df[0] == palabra].index[0])
             for palabra in semillas_nah]

print(len(index_esp), len(index_nah))

# obtener listas de semillas de español y náhuatl de evaluación
#semillas_esp_test = list(set(test_set["esp"].values))
#semillas_nah_test = list(set(test_set["nah"].values))

def get_vectors(dataframe, index, format=np.float64):
    """
    Retorna los vectores dentro del dataframe.
    
    Args:
        dataframe (Pandas.dataframe): Contiene las palabras y su representación vectorial.
        index (list): Contiene los índices que se necesitan del dataframe.
        format (numpy format): Tipo flotante. Default float64.
    
    Returns:
        Numpy array: Matriz con representaciones vectoriales.
    """

    return np.array([(dataframe.iloc[_].loc[1::])
                     for _ in index]).astype(format)

# Obtener representaciones vectoriales de las semillas.
es_vectores = get_vectors(es_df, index_esp)
na_vectores = get_vectors(na_df, index_nah)
len(es_vectores)

es_vectores.shape[0]

#index_dummy=[np.random.randint(24) for _ in range(150)]
es_dummy = es_vectores
na_dummy = na_vectores
for i in range(es_dummy.shape[0]):
    es_vectores=np.vstack((es_vectores,es_dummy[i][::-1]))
    na_vectores=np.vstack((na_vectores,na_dummy[i][::-1]))

es_vectores.shape,na_vectores.shape

def next_batch(x,y, step, batch_size):
    """Función para obtener batches de un conjunto de datos
    
    Arguments:
        x {numpyarray} -- Conjunto de datos (inputs).
        y {numpyarray} -- Conjunto de datos (targets).
        step {int} -- Batches.
        batch_size {int} -- Tamaño del batch.
    
    Returns:
        numpyarray -- Subconjunto de tamaño batch_size.
    """

    return x[batch_size * step:batch_size * step + batch_size],y[batch_size * step:batch_size * step + batch_size]

"""# Entrenamiento"""

"""CHECKPOINT; solo cambiar epochs, la precisión es buena con este código...
19k
"""


tf.set_random_seed(42)
tf.reset_default_graph()
print("TensorFlow v{}".format(tf.__version__))
print(tf.test.gpu_device_name())

LEARNING_RATE = 1
EPOCHS = 35000
# Dimensión de vectores de entrada (número de neuronas en capa de entrada).
NODES_INPUT = es_vectores[0].size

# Número de neuronas en capas ocultas.
NODES_H1 = 300  #70 - 20 - 15
NODES_H2 = 150  # 42 - 20
NODES_H3 = 200  # 42 - 20
NODES_H4 = 200  # 42 - 20
NODES_OUTPUT = na_vectores[0].size
XAVIER_INIT = True
HE_INIT = np.sqrt(2 / NODES_INPUT)
BATCHSIZE = 124
INIT = {"HE":tf.keras.initializers.he_uniform(seed=42),
        "XAVIER":tf.contrib.layers.xavier_initializer(dtype=tf.float64,seed=42)}

DROP = 1

model = "model2250"


with tf.name_scope('input'):
    # El valor None indica que se puede modificar la dimensión de los tensores
    # por si se usan todos los vectores o batches.
    X = tf.placeholder(shape=[None, NODES_INPUT],dtype=tf.float64, name='input_es')
    y = tf.placeholder(shape=[None, NODES_OUTPUT],dtype=tf.float64, name='target_na')

kprob = tf.placeholder(tf.float64,name='dropout_prob')
    
def activation_function(layer, act, name, alpha=tf.constant(0.2, dtype=tf.float64)):
    if act == "leaky_relu":
        return tf.nn.leaky_relu(layer, alpha, name=name)
    elif act == "softmax":
        return tf.nn.softmax(layer, name=name)
    elif act == "sigmoid":
        return tf.nn.sigmoid(layer, name=name)
    elif act == "tanh":
        return tf.nn.tanh(layer, name=name)
    elif act == "elu":
        return tf.nn.elu(layer,name=name)
    return tf.nn.relu(layer, name=name)


# Se definen las capas.
'''W1 = tf.get_variable(name="W1", shape=[NODES_INPUT, NODES_OUTPUT], dtype=tf.float64,
                                initializer=tf.contrib.layers.xavier_initializer(dtype=tf.float64),
                                use_resource=True,
                                regularizer = tf.contrib.layers.l2_regularizer(scale=0.1))
b1 = tf.Variable(tf.constant(0.1, shape=[NODES_OUTPUT], dtype=tf.float64), name="b1")
fc1 = activation_function(tf.nn.xw_plus_b(X,W1,b1), "relu", "fc1")


W2 = tf.get_variable(name="W2", shape=[NODES_H1, NODES_H2], dtype=tf.float64,
                                initializer=tf.contrib.layers.xavier_initializer(dtype=tf.float64),
                                use_resource=True,
                                regularizer = tf.contrib.layers.l2_regularizer(scale=0.1))
b2 = tf.Variable(tf.constant(0.1, shape=[NODES_H2], dtype=tf.float64), name="b2")
fc2 = activation_function(tf.nn.xw_plus_b(fc1,W2,b2), "relu", "fc2")



b_na = tf.Variable(tf.constant(0.1, shape=[NODES_OUTPUT], dtype=tf.float64), name="b_na")
nah_predicted = tf.nn.xw_plus_b(fc1,tf.transpose(W1),b_na)'''
'''
fc1 = tf.layers.dense(X,NODES_H1,activation=tf.nn.sigmoid,
                      kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.1),
                      kernel_initializer=tf.contrib.layers.xavier_initializer(dtype=tf.float64))

fc2 = tf.layers.dense(fc1,NODES_H2,activation=tf.nn.sigmoid,
                      kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.1),
                      kernel_initializer=tf.contrib.layers.xavier_initializer(dtype=tf.float64))

fc3 = tf.layers.dense(fc1,NODES_H3,activation=tf.nn.sigmoid,
                      kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.1),
                      kernel_initializer=tf.contrib.layers.xavier_initializer(dtype=tf.float64))
nah_predicted = tf.layers.dense(fc1,NODES_OUTPUT,activation=None,
                               kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.1),
                               kernel_initializer=tf.contrib.layers.xavier_initializer(dtype=tf.float64))

'''
W1 = tf.get_variable(name="W1", shape=[NODES_INPUT, NODES_H1], dtype=tf.float64,
                                initializer=INIT["HE"],
                                use_resource=True,
                                regularizer = tf.contrib.layers.l2_regularizer(scale=0.1)
                    )
#W1 = tf.Variable(tf.truncated_normal([NODES_INPUT, NODES_H1], stddev=0.01,dtype=tf.float64), name="W1")
b1 = tf.Variable(tf.zeros(shape=[NODES_H1], dtype=tf.float64), name="b1")
fc1 = activation_function(tf.nn.xw_plus_b(X,W1,b1), "elu", "fc1")
fc1 = tf.nn.dropout(fc1,kprob,seed=42)


'''W2 = tf.get_variable(name="W2",shape=[NODES_H1,NODES_H2],dtype=tf.float64,
                                initializer=INIT["HE"],
                                use_resource=True,
                                regularizer=tf.contrib.layers.l2_regularizer(scale=0.1)
                                )
b2 = tf.Variable(tf.zeros(shape=[NODES_H2], dtype=tf.float64), name="b2")
fc2 = activation_function(tf.nn.xw_plus_b(fc1,W2,b2),"elu","fc2")
fc2 = tf.nn.dropout(fc2,kprob,seed=42)'''


W_na = tf.get_variable(name="W_na",shape=[NODES_H1,NODES_OUTPUT],dtype=tf.float64,
                                initializer=INIT["XAVIER"],
                                use_resource=True,
                                regularizer=tf.contrib.layers.l2_regularizer(scale=0.1)
                                )
b_na = tf.Variable(tf.zeros(shape=[NODES_OUTPUT],dtype=tf.float64),name="b_na")
nah_predicted = tf.nn.xw_plus_b(fc1,W_na,b_na)




reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)
reg_constant = 0.01  # Choose an appropriate one.

loss = tf.reduce_mean(tf.squared_difference(nah_predicted, y), name="loss")

#loss = loss + reg_constant*sum(reg_losses)
tf.summary.scalar("loss", loss)


#optimiser = tf.train.MomentumOptimizer(learning_rate=LEARNING_RATE, momentum=0.5)
optimiser = tf.train.AdagradOptimizer(learning_rate=LEARNING_RATE)
#train_op = tf.train.AdagradOptimizer(learning_rate=LEARNING_RATE).minimize(loss)


# Compute gradients
gradients, variables = zip(*optimiser.compute_gradients(loss))

gradients, _ = tf.clip_by_global_norm(gradients, 5.0)

# Apply processed gradients to optimizer.
train_op = optimiser.apply_gradients(zip(gradients, variables))

# Accuracy 
with tf.name_scope('accuracy'):
    with tf.name_scope('correct_prediction'):
        # Se compara salida de la red neuronal con el vector objetivo.
        correct_prediction = tf.equal(tf.argmax(nah_predicted, 1), tf.argmax(y, 1))
    with tf.name_scope('accuracy'):
        # Se calcula la precisión.
        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float64))
    tf.summary.scalar('accuracy', accuracy)

LOGPATH = ".logs/model"
print("logpath:", LOGPATH)


# Se crea la sesión
sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))

# Se ponen los histogramas y valores de las gráficas en una sola variable.
summaryMerged = tf.summary.merge_all()

# Escribir a disco el grafo generado y las gráficas para visualizar en TensorBoard.
writer = tf.summary.FileWriter(LOGPATH, sess.graph)

# Se inicializan los valores de los tensores.
init = tf.global_variables_initializer()

# Add ops to save and restore all the variables.
saver = tf.train.Saver()

# Ejecutando sesión
sess.run(init)



for j in range(EPOCHS):
    
    #for i in range(0,4):
    #es_batch, na_batch = next_batch(es_vectores,na_vectores, i, 128)
    _loss, _, sumOut = sess.run([loss, train_op,summaryMerged], feed_dict={
                        X: es_vectores, y: na_vectores,kprob:DROP})

    writer.add_summary(sumOut, j)

    if (j % 700) == 0:
        #train_accuracy = accuracy.eval(session=sess, feed_dict={X: es_vectores,y: na_vectores})
        print("Epoch:", j, "/", EPOCHS, "\tLoss:",_loss)#, "\tAccuracy:", train_accuracy)
        

SAVE_PATH = "./"+model+".ckpt"
print("save path",SAVE_PATH)
save_model = saver.save(sess, SAVE_PATH)
print("Model saved in file: %s", SAVE_PATH)
writer.close()

"""# Descargar modelo generado"""

files.download("/content/checkpoint")

files.download("/content/"+model+".ckpt.meta")

files.download("/content/"+model+".ckpt.index")

files.download("/content/"+model+".ckpt.data-00000-of-00001")
